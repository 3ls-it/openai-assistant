#!/usr/bin/env python3 
"""
openai-assistant
Copyright 2024, 2025 J Adams jfa63[at]duck[dot]com
Released under the 2-Clause BSD license.
"""
__version__ = 'v1.0.6'
__author__ = 'J. Adams jfa63[at]duck[dot]com'

# Builtin imports 
from datetime import datetime
import os
import sys
import traceback
import warnings

# OpenAI imports 
import openai
from openai import OpenAI
from openai import AssistantEventHandler
from openai.types.vector_stores.vector_store_file_batch import VectorStoreFileBatch

# Rich imports for output formatting 
from rich import print
from rich.console import Console
from rich.markdown import Markdown

# Imports for prompt inputs 
from typing_extensions import override
from prompt_toolkit import PromptSession
from prompt_toolkit.key_binding import KeyBindings

# Imports for configuration loading  
import runpy
from pathlib import Path


## Some global definitions 
# Define home directory and config paths
homedir     = str(Path.home()) + "/"
CONFIG_DIR  = Path(homedir) / ".openai"
CONFIG_PATH = CONFIG_DIR / "settings.py"
logfile = homedir + ".openai/chat_log.txt"

# Create a PromptSession object
session = PromptSession(multiline=True)
# Define custom key bindings
kb = KeyBindings()
# Create a Rich Console for response display
console = Console()

# Required and optional settings 
REQUIRED  = ("MODEL", "TEMP", "YOUR_NAME", "ASSISTANT_NAME", "OPENAI_API_KEY")
DEFAULTS  = {"MODEL": "gpt-4.1", "TEMP": 0.3, "YOUR_NAME": "User", "ASSISTANT_NAME": "OpenAI"}
OPTIONAL  = ("FURTHER_INSTRUCTIONS", "VECTOR_STORE_ID", "ASSISTANT_ID")

# Turn off DepricationWarnings 
# We know Assistans API is to be depricated
warnings.filterwarnings("ignore", category=DeprecationWarning)



## Helper functions 
# Helper to load configuration settings 
def load_settings():
    if CONFIG_PATH.is_file():
        return runpy.run_path(str(CONFIG_PATH))
    return {}
# End load_settings() 


# Helper to update a single key in ~/.openai/settings.py
def write_setting(key: str, val):
    lines = []
    found = False
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        for line in f:
            if line.startswith(f"{key} ="):
                # overwrite the existing line
                if isinstance(val, str):
                    lines.append(f"{key} = '{val}'\n")
                else:
                    lines.append(f"{key} = {val}\n")
                found = True
            else:
                lines.append(line)
    if not found:
        # append at end
        if isinstance(val, str):
            lines.append(f"{key} = '{val}'\n")
        else:
            lines.append(f"{key} = {val}\n")
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        f.writelines(lines)
# End write_settings() 


# Define a key combination (e.g., Tab+Enter) to submit input
@kb.add('tab', 'enter')
def _(event):
    event.app.current_buffer.validate_and_handle()
# End _() 


# Upload file 
def file_upload(client, vector_store, filename: str) -> VectorStoreFileBatch:
    # Skip if the file is empty or missing
    if not os.path.isfile(filename) or os.path.getsize(filename) == 0:
        print(f"Skipping empty file upload: {filename}", "[green]ENTER[/] to continue")
        input()
        return False
    file_paths = [filename]
    file_streams = [open(path, "rb") for path in file_paths]

    # Upload files to the vector store
    file_batch = client.vector_stores.file_batches.upload_and_poll(
        vector_store_id=vector_store.id, files=file_streams
    )

    for fs in file_streams:
        fs.close()

    #print(f"Uploaded: [blue]{filename}[/blue]\n" + file_batch.status)

    return file_batch
# End file_upload()  



# Extend the EventHandler class overriding two methods   
# to handle the events in the response stream. 
# Added text attribute 
class EventHandler(AssistantEventHandler):    
    text: str
    
    def __init__(self):
        # Initialize handler to set up internal state
        super().__init__()
        # Accumulate streamed text here
        self.text = ''

    @override
    def on_text_created(self, text) -> None:
        # Write to logfile. 
        lf.write(f'{assistant_name}: ')
    
    @override
    def on_text_delta(self, delta, snapshot):
        resp = str(delta.value)
        # Write delta to logfile. 
        lf.write(resp)
        # Concatenate deltas for display. 
        self.text += resp
    
    def on_tool_call_created(self, tool_call):
        print(f'\n> {tool_call.type}\n', flush=True)

    def on_tool_call_delta(self, delta, snapshot):
        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end='', flush=True)
            if delta.code_interpreter.outputs:
                print('\n\nOutput >', flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == 'logs':
                        print(f'\n{output.logs}', flush=True)
# End class EventHandler() 



## Get and load  settings, prompt for missing settings  
settings = load_settings()
updated  = False

# Prompt for missing required settings
for key in REQUIRED:
    if key not in settings or (key == "OPENAI_API_KEY" and not settings.get(key)):
        if key in DEFAULTS:
            settings[key] = DEFAULTS[key]
            print(f"No {key} found. Using default: {settings[key]}")
        else:
            settings[key] = input(f"Enter {key}: ").strip()
        updated = True

# Ensure optional settings exist
for key in OPTIONAL:
    if key not in settings:
        settings[key] = ""
        updated = True

# Write settings file if needed
if updated or not CONFIG_PATH.exists():
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        f.write("# ~/.openai/settings.py  (auto-generated by openai-assistant)\n")
        for key in REQUIRED + OPTIONAL:
            val = settings[key]
            if isinstance(val, str):
                f.write(f"{key} = '{val}'\n")
            else:
                f.write(f"{key} = {val}\n")
    print(f"Wrote settings to {CONFIG_PATH}")

# Expose settings as module-level names
MODEL                = settings["MODEL"]
TEMP                 = settings["TEMP"]
OPENAI_API_KEY       = settings["OPENAI_API_KEY"]
YOUR_NAME            = settings["YOUR_NAME"]
ASSISTANT_NAME       = settings["ASSISTANT_NAME"]
FURTHER_INSTRUCTIONS = settings["FURTHER_INSTRUCTIONS"]
VECTOR_STORE_ID      = settings["VECTOR_STORE_ID"]
ASSISTANT_ID         = settings["ASSISTANT_ID"]

gpt_model = MODEL
temp = TEMP
your_name = YOUR_NAME
assistant_name = ASSISTANT_NAME
api_key = OPENAI_API_KEY
instructions = f"You are {assistant_name}, {your_name}'s personal AI assistant."

# Open log file in append mode 
lf = open(logfile, 'a', encoding='utf-8')



def main():
    # Clear terminal 
    os.system('clear')

    # Get current date and time
    now = datetime.now()
    # Format as '9 June 2025 9:52pm'
    tstamp = now.strftime('%-d %B %Y %-I:%M%p').lower()
    # Write timestamp to log file for session date 
    lf.write(f"\n\nSession time stamp: {tstamp}")

    # Instantiate an OpenAI object. 
    try:
        client = OpenAI(api_key=api_key)
        # Set up or retrieve an Assistant to handle requests.
        if ASSISTANT_ID:
            assistant = client.beta.assistants.retrieve(
                assistant_id=ASSISTANT_ID
            )
        else:
            assistant = client.beta.assistants.create(
                instructions=instructions + FURTHER_INSTRUCTIONS,
                name=assistant_name,
                tools=[{"type": "code_interpreter"}, {"type": "file_search"}],
                model=gpt_model,
                temperature=temp
            )
            print(f"[bold green]Created new assistant with ID {assistant.id}[/bold green]")
            # Offer to write ASSISTANT_ID back to settings.py
            resp = input("Save ASSISTANT_ID to settings.py? [Y/n] ").strip().lower()
            if resp in ("", "y", "yes"):
                write_setting("ASSISTANT_ID", assistant.id)
                print(f"Updated ASSISTANT_ID in {CONFIG_PATH}")
            else:
                print("ASSISTANT_ID not saved; will create new next run.")

        # Set up or retrieve a Vector Store to hold the chat history.
        if VECTOR_STORE_ID:
            # Retrieve existing vector store by ID
            vector_store = client.vector_stores.retrieve(vector_store_id=VECTOR_STORE_ID)
        else:
            vector_store = client.vector_stores.create(name="Chat History")
            print(f"[bold green]Created new vector store with ID {vector_store.id}[/bold green]")
            # Offer to write VECTOR_STORE_ID back to settings.py
            resp = input("Save VECTOR_STORE_ID to settings.py? [Y/n] ").strip().lower()
            if resp in ("", "y", "yes"):
                write_setting("VECTOR_STORE_ID", vector_store.id)
                print(f"Updated VECTOR_STORE_ID in {CONFIG_PATH}")
            else:
                print("VECTOR_STORE_ID not saved; will create new next run.")

        # Upload chat log 
        with console.status("[b blue]Uploading chat log file[/]\n", spinner='simpleDots'):
            file_upload(client, vector_store, logfile)
        os.system('clear')

        # Update Assistant object with vector store
        assistant = client.beta.assistants.update(
                assistant_id=assistant.id,
                tool_resources={"file_search":
                                {"vector_store_ids": [vector_store.id]}},
        )
    
        # Create Thread instance. 
        thread = client.beta.threads.create()

    except openai.APIConnectionError as e:
        print("[red]The server could not be reached[/]")
        print(e)  # an underlying Exception, likely raised within httpx.
        sys.exit(1)
    except openai.AuthenticationError as e:
        print("[red]Check your API key or token and make sure it is correct and active.[/]")
        print(e)  # an underlying Exception, likely raised within httpx.
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected error: {e}")
        traceback.print_exc()
        sys.exit(1)

    # We take user input, create a message,  
    # create a Run, then stream the whole of the response. 
    print(f'[u #f5d676]Openai Assistant client {__version__}[/]\n')
    print('[b blue]Enter "TAB+ENTER" to submit[/]')
    print('[b blue]Enter "Done" or "done" to quit session\n[/]')
    console.rule(f"[#f5d676]{tstamp}[/]", style='white')
    print(f'[#f5d676]\nHow can I help you {your_name}?\n[/]')

    while True:
        # Take user input. 
        try:
            # Get user input with custom key bindings
            query = session.prompt('>> ', key_bindings=kb)
            # Display user query in Markdown format using rich
            query_md = Markdown(query)
            print("\n[blue b]Me:[/]")
            console.print(query_md)
        except (EOFError, KeyboardInterrupt):
            break

        if query.strip().lower() in ("done"):
            print(f'\n[#f5d676]Bye! -{assistant_name}[/]')
            break

        if query.strip().lower() == "upload":
            fname = input('Filename realative to ~/: ')
            if os.path.isfile(homedir + fname) is True:
                print("You have chosen the file:")
                print(homedir + fname)
                resp = input("Is this correct? [Y/n] ")
                # Handle typical "yes" responses 
                if resp.lower() in ("y", "yes", ""):
                    filename = homedir + fname
                    try:
                        batch = file_upload(client, vector_store, filename)
                        print(f"Uploaded: [blue]{filename}[/blue]\n" + batch.status)
                        # Update Assistant object 
                        assistant = client.beta.assistants.update(
                              assistant_id=assistant.id,
                              tool_resources={"file_search":
                                             {"vector_store_ids": [vector_store.id]}},
                        )
                        # Prevent sending "Upload" as a chat message
                        continue
                    except openai.BadRequestError as e:
                        print("Recieved a BadRequestError from OpenAI.")
                        print("This could mean that the file type is unsupported.\n")
                        print(e)
                        continue
                else:
                    print("[red]Cancelled[/red]\n\n")
                    continue
            else:
                print("The file:")
                print(homedir + fname)
                print("does not exist, has been moved, or is a directory.\n\n")
                continue

        try:
            # Write the query to logfile. 
            quest = "\n\nMe: " + query
            lf.write(quest+"\n")

            # Create a message from the user query. 
            client.beta.threads.messages.create(
                    thread_id=thread.id,
                    role="user",
                    content=query
            )

            # Then, we use the `stream` SDK helper 
            # with the `EventHandler` class to create the Run 
            # and stream the response. We then render and display
            # the response using Console() and Markdown() from rich. 
            with client.beta.threads.runs.stream(
                    thread_id=thread.id,
                    assistant_id=assistant.id,
                    event_handler=EventHandler(),
                ) as stream:
                        with console.status("[#f5d676]Thinking[/]", spinner='simpleDots'):
                            stream.until_done()
                        resp_md = Markdown(stream.text)
                        print(f'\n[#f5d676 b]{assistant_name}:[/]')
                        console.print(resp_md)
                        lf.write("\n")
                        print("\n")

        except openai.APIConnectionError as e:
            print("The server could not be reached")
            print(e)  # an underlying Exception, likely raised within httpx.
        except openai.RateLimitError as e:
            print("A 429 status code was received; we should back off a bit.")
            print(e)
        except openai.APIStatusError as e:
            print("A non-200-range status code was received")
            print(e.status_code)
            print(e.response)
        # End try 
    # End While  

    lf.flush()
    lf.close()
    print()
# End main()  


if __name__ == '__main__':
    main()
